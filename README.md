# R1 - Lightweight GRPO Training Framework

A lightweight Group-based Preference Optimization (GRPO) training framework built on Hugging Face transformers, optimized for reasoning model training.

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

---

## English

### Quick Start

```bash
# Setup environment
conda create -n r1 python=3.10
conda activate r1
pip install -e .

# Run training demo
python train_enhanced_demo.py
```

### Features

- **Enhanced GRPO Trainer**: Extended from `trl.GRPOTrainer` with real-time evaluation
- **Modular Reward System**: Multiple reward functions (accuracy, format, reasoning, etc.)
- **YAML Configuration**: Flexible training parameter management
- **Built-in Utilities**: Logging, formatting, and data processing tools

### Enhanced Features

**ğŸš€ What makes this "Enhanced"?**

Compared to standard GRPO implementations, this framework provides:

| Feature | Standard GRPO | Enhanced GRPO |
|---------|---------------|---------------|
| **Reward Functions** | 1-2 basic rewards | **6 specialized rewards** with customizable weights |
| **Training Monitoring** | Basic logging | **Real-time evaluation** every N steps |
| **Progress Tracking** | Step counter only | **Detailed statistics** + ETA estimation |
| **Evaluation** | End-of-training only | **Pre-training baseline** + continuous monitoring |
| **Reward Quality** | Simple scoring | **Multi-dimensional scoring** with bonuses |
| **Configurability** | Fixed parameters | **Flexible YAML configuration** |

**Key Enhancements:**

1. **Advanced Reward System**: 6 reward functions with intelligent bonuses
   - `accuracy`: Math verification + confidence detection
   - `format`: Structure validation + completeness scoring  
   - `reasoning_steps`: Logic quality assessment
   - `tag_count`: Smart tag validation with penalty system
   - `length`: Optimal response length control
   - `mathematical_rigor`: Mathematical reasoning evaluation

2. **Real-time Training Insights**:
   - Live evaluation during training
   - Progress estimation with ETA
   - Best metric tracking
   - Comprehensive training summary

3. **Modular Architecture**: Easy to extend with new reward functions and evaluation metrics

### Configuration

Basic example:
```yaml
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
dataset_name: open-r1/Mixture-of-Thoughts
learning_rate: 3.0e-6
num_train_epochs: 2
# All available reward functions
reward_funcs: [accuracy, format, reasoning_steps, tag_count, length, mathematical_rigor]
reward_weights: [4.0, 1.0, 2.0, 1.0, 0.8, 1.5]  # Customizable weights for each function
```

### Project Structure

```
src/r1/
â”œâ”€â”€ enhanced_grpo_trainer.py    # Main trainer implementation
â”œâ”€â”€ rewards.py                  # Reward function library
â”œâ”€â”€ configs.py                  # Configuration classes
â”œâ”€â”€ grpo.py                     # Training script
â””â”€â”€ utils/                      # Utility modules
    â”œâ”€â”€ logging.py
    â”œâ”€â”€ formatting.py
    â””â”€â”€ data.py
```

### Reward Functions

| Function | Description |
|----------|-------------|
| `accuracy` | Answer correctness using math-verify |
| `format` | Tag structure validation |
| `reasoning_steps` | Logical reasoning quality |
| `tag_count` | Required tag presence |
| `length` | Response length control |
| `mathematical_rigor` | Math reasoning evaluation |

### Training Scripts

- `train_enhanced_demo.py` - Demo with enhanced features
- `src/r1/grpo.py` - Standard GRPO training
- `src/r1/sft.py` - Supervised fine-tuning

---

## ä¸­æ–‡

### å¿«é€Ÿå¼€å§‹

```bash
# ç¯å¢ƒé…ç½®
conda create -n r1 python=3.10
conda activate r1
pip install -e .

# è¿è¡Œè®­ç»ƒæ¼”ç¤º
python train_enhanced_demo.py
```

### ä¸»è¦åŠŸèƒ½

- **å¢å¼ºGRPOè®­ç»ƒå™¨**: åŸºäº`trl.GRPOTrainer`æ‰©å±•ï¼Œæ”¯æŒå®æ—¶è¯„ä¼°
- **æ¨¡å—åŒ–å¥–åŠ±ç³»ç»Ÿ**: å¤šç§å¥–åŠ±å‡½æ•°ï¼ˆå‡†ç¡®æ€§ã€æ ¼å¼ã€æ¨ç†ç­‰ï¼‰
- **YAMLé…ç½®ç®¡ç†**: çµæ´»çš„è®­ç»ƒå‚æ•°é…ç½®
- **å†…ç½®å·¥å…·åº“**: æ—¥å¿—è®°å½•ã€æ ¼å¼åŒ–ã€æ•°æ®å¤„ç†å·¥å…·

### å¢å¼ºåŠŸèƒ½

**ğŸš€ "å¢å¼º"ä½“ç°åœ¨å“ªé‡Œï¼Ÿ**

ç›¸æ¯”æ ‡å‡†GRPOå®ç°ï¼Œæœ¬æ¡†æ¶æä¾›ï¼š

| åŠŸèƒ½ | æ ‡å‡†GRPO | å¢å¼ºGRPO |
|------|---------|---------|
| **å¥–åŠ±å‡½æ•°** | 1-2ä¸ªåŸºç¡€å¥–åŠ± | **6ä¸ªä¸“é—¨å¥–åŠ±**ï¼Œå¯è‡ªå®šä¹‰æƒé‡ |
| **è®­ç»ƒç›‘æ§** | åŸºç¡€æ—¥å¿— | **å®æ—¶è¯„ä¼°**ï¼Œæ¯Næ­¥ä¸€æ¬¡ |
| **è¿›åº¦è¿½è¸ª** | ä»…æ­¥æ•°ç»Ÿè®¡ | **è¯¦ç»†ç»Ÿè®¡** + ETAé¢„ä¼° |
| **è¯„ä¼°æ–¹å¼** | ä»…è®­ç»ƒç»“æŸæ—¶ | **è®­ç»ƒå‰åŸºçº¿** + æŒç»­ç›‘æ§ |
| **å¥–åŠ±è´¨é‡** | ç®€å•æ‰“åˆ† | **å¤šç»´åº¦æ‰“åˆ†**ï¼Œå¸¦å¥–åŠ±æœºåˆ¶ |
| **å¯é…ç½®æ€§** | å›ºå®šå‚æ•° | **çµæ´»YAMLé…ç½®** |

**æ ¸å¿ƒå¢å¼ºåŠŸèƒ½ï¼š**

1. **é«˜çº§å¥–åŠ±ç³»ç»Ÿ**: 6ä¸ªå¥–åŠ±å‡½æ•°ï¼Œå«æ™ºèƒ½å¥–åŠ±æœºåˆ¶
   - `accuracy`: æ•°å­¦éªŒè¯ + ç½®ä¿¡åº¦æ£€æµ‹
   - `format`: ç»“æ„éªŒè¯ + å®Œæ•´æ€§è¯„åˆ†
   - `reasoning_steps`: é€»è¾‘è´¨é‡è¯„ä¼°
   - `tag_count`: æ™ºèƒ½æ ‡ç­¾éªŒè¯ï¼Œå«æƒ©ç½šæœºåˆ¶
   - `length`: æœ€ä¼˜å“åº”é•¿åº¦æ§åˆ¶
   - `mathematical_rigor`: æ•°å­¦æ¨ç†è¯„ä¼°

2. **å®æ—¶è®­ç»ƒæ´å¯Ÿ**:
   - è®­ç»ƒä¸­å®æ—¶è¯„ä¼°
   - è¿›åº¦é¢„ä¼°å’ŒETA
   - æœ€ä½³æŒ‡æ ‡è¿½è¸ª
   - å…¨é¢è®­ç»ƒæ€»ç»“

3. **æ¨¡å—åŒ–æ¶æ„**: æ˜“äºæ‰©å±•æ–°å¥–åŠ±å‡½æ•°å’Œè¯„ä¼°æŒ‡æ ‡

### é…ç½®ç¤ºä¾‹

åŸºç¡€é…ç½®:
```yaml
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
dataset_name: open-r1/Mixture-of-Thoughts
learning_rate: 3.0e-6
num_train_epochs: 2
# æ‰€æœ‰å¯ç”¨çš„å¥–åŠ±å‡½æ•°
reward_funcs: [accuracy, format, reasoning_steps, tag_count, length, mathematical_rigor]
reward_weights: [4.0, 1.0, 2.0, 1.0, 0.8, 1.5]  # æ¯ä¸ªå‡½æ•°å¯è‡ªå®šä¹‰æƒé‡
```

### é¡¹ç›®ç»“æ„

```
src/r1/
â”œâ”€â”€ enhanced_grpo_trainer.py    # ä¸»è¦è®­ç»ƒå™¨å®ç°
â”œâ”€â”€ rewards.py                  # å¥–åŠ±å‡½æ•°åº“
â”œâ”€â”€ configs.py                  # é…ç½®ç±»
â”œâ”€â”€ grpo.py                     # è®­ç»ƒè„šæœ¬
â””â”€â”€ utils/                      # å·¥å…·æ¨¡å—
    â”œâ”€â”€ logging.py
    â”œâ”€â”€ formatting.py
    â””â”€â”€ data.py
```

### å¥–åŠ±å‡½æ•°

| å‡½æ•° | è¯´æ˜ |
|------|------|
| `accuracy` | ä½¿ç”¨math-verifyéªŒè¯ç­”æ¡ˆæ­£ç¡®æ€§ |
| `format` | æ ‡ç­¾ç»“æ„éªŒè¯ |
| `reasoning_steps` | é€»è¾‘æ¨ç†è´¨é‡è¯„ä¼° |
| `tag_count` | å¿…éœ€æ ‡ç­¾å­˜åœ¨æ€§æ£€æŸ¥ |
| `length` | å“åº”é•¿åº¦æ§åˆ¶ |
| `mathematical_rigor` | æ•°å­¦æ¨ç†è¯„ä¼° |

### è®­ç»ƒè„šæœ¬

- `train_enhanced_demo.py` - å¢å¼ºåŠŸèƒ½æ¼”ç¤º
- `src/r1/grpo.py` - æ ‡å‡†GRPOè®­ç»ƒ
- `src/r1/sft.py` - ç›‘ç£å¾®è°ƒ

## License

Apache-2.0
