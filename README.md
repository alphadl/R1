<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>R1 - Lightweight GRPO Inference Model Training Framework</title>
    <style>
        body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; }
        .lang-toggle { position: fixed; top: 20px; right: 20px; z-index: 1000; }
        .lang-btn { 
            background: #0969da; 
            color: white; 
            border: none; 
            padding: 8px 16px; 
            border-radius: 6px; 
            cursor: pointer; 
            margin: 0 2px;
            font-size: 14px;
        }
        .lang-btn:hover { background: #0860ca; }
        .lang-btn.active { background: #2da44e; }
        .lang-content { display: none; }
        .lang-content.active { display: block; }
        .emoji { font-size: 1.2em; }
    </style>
</head>
<body>

<div class="lang-toggle">
    <button class="lang-btn active" onclick="switchLang('en')">English</button>
    <button class="lang-btn" onclick="switchLang('zh')">ä¸­æ–‡</button>
</div>

<!-- English Version -->
<div id="lang-en" class="lang-content active">

# R1 - Lightweight GRPO Inference Model Training Framework

ğŸš€ **GRPO (Group-based Preference Optimization) Training Framework Optimized for Inference Tasks**

Built on the Hugging Face ecosystem, focused on training high-quality inference models that excel at mathematical reasoning, logical reasoning, and other complex cognitive tasks.

## âœ¨ Core Features

### ğŸ§  Enhanced GRPO Training
- **Complete GRPO Implementation**: Enhanced trainer based on `trl.GRPOTrainer`
- **Real-time Evaluation System**: Automatic model performance evaluation during training
- **Reject Sampling Mechanism**: Intelligent resampling to improve generation quality
- **Modular Design**: Pluggable reward functions and configuration system

### ğŸ¯ Multi-dimensional Reward Functions
- **Answer Accuracy** (`accuracy`): Precise verification using `math-verify`
- **Format Compliance** (`format`): Validates `<think>` and `<answer>` tag structure
- **Reasoning Steps** (`reasoning_steps`): Evaluates logical reasoning process quality
- **Tag Completeness** (`tag_count`): Ensures correct tag usage
- **Text Length** (`length`): Controls reasonable generation length
- **Mathematical Rigor** (`mathematical_rigor`): Specialized evaluation for mathematical reasoning

### âš™ï¸ Flexible Configuration System
- **YAML-Driven**: All training parameters managed through configuration files
- **Adjustable Weights**: Independent weight settings for each reward function
- **Dynamic Loading**: Smart separation of training and model parameters
- **Environment Adaptation**: Supports multiple accelerators and distributed training

### ğŸ”§ Developer-Friendly
- **Complete Toolchain**: Logging, formatting, data processing utilities
- **Testing Framework**: Comprehensive functional testing and validation
- **Debug Support**: Detailed logging and debug mode

## ğŸ“¦ Quick Start

### Environment Setup
```bash
# Requires Python 3.10.9+
conda create -n r1 python=3.10
conda activate r1

# Clone the project
git clone <repository-url>
cd r1

# Install dependencies (PyTorch ecosystem based)
pip install -e .
```

### Basic Training
```bash
# Quick start with preset configuration
python train_enhanced_demo.py

# Custom configuration file
python train_enhanced_demo.py --config recipes/Qwen2.5-1.5B-Instruct/grpo/config_enhanced_demo.yaml

# Debug mode (no actual training)
python train_enhanced_demo.py --debug --dry-run
```

### Advanced Training
```bash
# Using native TRL interface
python src/r1/grpo.py --config <config_file>

# SFT pre-training
python src/r1/sft.py --config <sft_config_file>
```

## ğŸ“‹ Configuration Details

### Basic Configuration Example
```yaml
# Model settings
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
torch_dtype: bfloat16

# Dataset configuration
dataset_name: open-r1/Mixture-of-Thoughts
dataset_prompt_column: prompt

# Training parameters
learning_rate: 3.0e-6
num_train_epochs: 2
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
max_completion_length: 2048

# Basic reward functions
reward_funcs:
  - accuracy
  - format
  - tag_count
reward_weights: [1.0, 1.0, 1.0]
```

### Enhanced Features Configuration
```yaml
# Real-time evaluation ğŸ”
eval_steps: 50                    # Evaluate every 50 steps
eval_on_start: true              # Evaluate before training
max_eval_samples: 100            # Evaluation sample limit
eval_dataset_name: gsm8k         # Independent evaluation dataset

# Reject sampling ğŸ²
use_reject_sampling: true        # Enable reject sampling
max_resample_attempts: 3         # Maximum retry attempts
reject_sampling_threshold: 0.6   # Quality threshold

# Multi-dimensional rewards ğŸ†
reward_funcs:
  - accuracy           # Answer accuracy
  - format            # Format compliance
  - reasoning_steps   # Reasoning quality
  - mathematical_rigor # Mathematical rigor
reward_weights: [4.0, 1.0, 2.0, 1.5]  # Weight configuration

# Performance optimization âš¡
torch_empty_cache_steps: 5       # GPU cache cleanup frequency
gradient_checkpointing: true     # Gradient checkpointing
use_liger_kernel: true          # Liger optimization kernel

# Monitoring & logging ğŸ“Š
wandb_project: R1-Training
log_completion_details: true
save_completions: false
debug_mode: true
```

## ğŸ—ï¸ Project Architecture

```
r1/
â”œâ”€â”€ ğŸ“ src/r1/                    # Core modules
â”‚   â”œâ”€â”€ ğŸ“„ configs.py            # Configuration classes (16KB, 448 lines)
â”‚   â”œâ”€â”€ ğŸ“„ enhanced_grpo_trainer.py  # Enhanced GRPO trainer (10KB, 284 lines)
â”‚   â”œâ”€â”€ ğŸ“„ trainer.py            # Trainer interface
â”‚   â”œâ”€â”€ ğŸ“„ rewards.py            # Reward function library (13KB, 354 lines)
â”‚   â”œâ”€â”€ ğŸ“„ callbacks.py          # Training callbacks
â”‚   â”œâ”€â”€ ğŸ“„ grpo.py               # GRPO training script (6.5KB, 183 lines)
â”‚   â”œâ”€â”€ ğŸ“„ sft.py                # SFT training script (6KB, 172 lines)
â”‚   â””â”€â”€ ğŸ“ utils/                # Utility library
â”‚       â”œâ”€â”€ ğŸ“„ logging.py        # Logging system (4.6KB, 152 lines)
â”‚       â”œâ”€â”€ ğŸ“„ formatting.py     # Text formatting (3.6KB, 152 lines)
â”‚       â”œâ”€â”€ ğŸ“„ data.py           # Data processing (2.6KB, 66 lines)
â”‚       â”œâ”€â”€ ğŸ“„ model_utils.py    # Model utilities (1.6KB, 43 lines)
â”‚       â”œâ”€â”€ ğŸ“„ import_utils.py   # Dependency checking (1.4KB, 55 lines)
â”‚       â”œâ”€â”€ ğŸ“„ callbacks.py      # Callback utilities (938B, 30 lines)
â”‚       â””â”€â”€ ğŸ“„ wandb_logging.py  # W&B integration (480B, 14 lines)
â”œâ”€â”€ ğŸ“ recipes/                  # Configuration templates
â”‚   â””â”€â”€ ğŸ“ Qwen2.5-1.5B-Instruct/grpo/
â”‚       â”œâ”€â”€ ğŸ“„ config_demo.yaml          # Basic configuration
â”‚       â””â”€â”€ ğŸ“„ config_enhanced_demo.yaml # Enhanced configuration
â”œâ”€â”€ ğŸ“„ train_enhanced_demo.py     # Training demo script (3KB, 92 lines)
â”œâ”€â”€ ğŸ“„ test_enhanced_features.py  # Feature testing script (4.8KB, 164 lines)
â””â”€â”€ ğŸ“„ setup.py                  # Package configuration (3.7KB, 125 lines)
```

## ğŸ“Š Reward Functions Explained

### ğŸ¯ accuracy - Answer Accuracy
- Uses `math-verify` and `latex2sympy2_extended` for precise verification
- Supports multiple mathematical expression formats
- Includes confidence-based reward mechanism

### ğŸ“ format - Format Compliance
- Validates completeness of `<think>` and `<answer>` tags
- Checks structural reasonableness of thinking process and answers
- Rewards clear logical segmentation

### ğŸ§  reasoning_steps - Reasoning Steps
- Detects step indicators (Step 1, First, Then, etc.)
- Evaluates use of logical transition words
- Supports both English and Chinese reasoning patterns

### ğŸ·ï¸ tag_count - Tag Completeness
- Precisely counts tag quantities
- Penalizes excessive or missing tags
- Ensures format consistency

### ğŸ“ length - Text Length
- Gaussian distribution-based length rewards
- Configurable target length ranges
- Avoids overly short or long generations

### ğŸ”¬ mathematical_rigor - Mathematical Rigor
- Detects mathematical terms and symbols
- Evaluates logical structure completeness
- Optimized specifically for mathematical reasoning tasks

## ğŸš€ Advanced Features

### Real-time Evaluation System
```python
# Automatically evaluate model during training
trainer = EnhancedGRPOTrainer(
    eval_dataset=eval_data,
    eval_steps=50,
    eval_on_start=True,
    max_eval_samples=100
)
```

### Reject Sampling Mechanism
```python
# Intelligent resampling to improve generation quality
completions = trainer._generate_completions_with_reject_sampling(
    prompts=prompts,
    max_resample_attempts=3,
    reject_sampling_threshold=0.6
)
```

### Modular Reward Functions
```python
# Create custom reward function combinations
from src.r1.rewards import get_reward_calculator

calculator = get_reward_calculator(
    reward_funcs=['accuracy', 'format', 'mathematical_rigor'],
    reward_weights=[4.0, 1.0, 2.0]
)
```

## ğŸ“ˆ Monitoring & Debugging

### Weights & Biases Integration
```yaml
# Complete experiment tracking
wandb_project: R1-Experiments
wandb_entity: your-team
run_name: qwen-grpo-v1
report_to: [wandb]
```

### Detailed Logging System
```python
# Multi-level logging
from src.r1.utils.logging import setup_logger, TrainingLogger

logger = setup_logger("r1_training", level=logging.DEBUG)
training_logger = TrainingLogger(log_file="training.log")
```

### Feature Testing
```bash
# Run comprehensive feature tests
python test_enhanced_features.py

# Example output:
# ğŸ¯ Overall Result: ğŸ‰ All tests passed!
```

## ğŸ› ï¸ Technology Stack

### Core Dependencies
- **PyTorch 2.6.0** - Deep learning framework
- **Transformers 4.52.3** - Model library
- **TRL 0.18.0** - Reinforcement learning training
- **Accelerate 1.4.0** - Distributed training
- **DeepSpeed 0.16.8** - Memory optimization

### Specialized Tools
- **math-verify 0.5.2** - Mathematical verification
- **latex2sympy2_extended** - LaTeX parsing
- **liger-kernel** - GPU optimization
- **bitsandbytes** - Quantized training

## ğŸ“ Development Guide

### Adding New Reward Functions
```python
def custom_reward(completions, **kwargs) -> list[float]:
    """Custom reward function"""
    # Implement evaluation logic
    return scores

# Register to system
REWARD_FUNCS_REGISTRY["custom"] = custom_reward
```

### Custom Configuration
```python
from src.r1.configs import GRPOScriptArguments

@dataclass
class CustomConfig(GRPOScriptArguments):
    custom_param: float = field(default=1.0)
```

## ğŸ¤ Contributing

1. **Fork the project** and create a feature branch
2. **Run tests** to ensure functionality: `python test_enhanced_features.py`
3. **Follow code style** using black and isort formatting
4. **Submit PR** with description of changes

## ğŸ“„ License

Apache License 2.0 - See LICENSE file for details

---

> ğŸ’¡ **Lightweight GRPO framework focused on inference tasks - making training high-quality reasoning models simple and efficient!**

</div>

<!-- Chinese Version -->
<div id="lang-zh" class="lang-content">

# R1 - è½»é‡çº§GRPOæ¨ç†æ¨¡å‹è®­ç»ƒæ¡†æ¶

ğŸš€ **ä¸“ä¸ºæ¨ç†ä»»åŠ¡ä¼˜åŒ–çš„GRPO (Group-based Preference Optimization) è®­ç»ƒæ¡†æ¶**

åŸºäºHugging Faceç”Ÿæ€ç³»ç»Ÿæ„å»ºï¼Œä¸“æ³¨äºè®­ç»ƒé«˜è´¨é‡çš„æ¨ç†æ¨¡å‹ï¼Œæ”¯æŒæ•°å­¦æ¨ç†ã€é€»è¾‘æ¨ç†ç­‰å¤æ‚ä»»åŠ¡ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸ§  å¢å¼ºç‰ˆGRPOè®­ç»ƒ
- **å®Œæ•´GRPOå®ç°**ï¼šåŸºäº`trl.GRPOTrainer`çš„å¢å¼ºç‰ˆè®­ç»ƒå™¨
- **å®æ—¶è¯„ä¼°ç³»ç»Ÿ**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­è‡ªåŠ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½
- **æ‹’ç»é‡‡æ ·æœºåˆ¶**ï¼šæ™ºèƒ½é‡é‡‡æ ·æå‡ç”Ÿæˆè´¨é‡
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šå¯æ’æ‹”çš„å¥–åŠ±å‡½æ•°å’Œé…ç½®ç³»ç»Ÿ

### ğŸ¯ å¤šç»´åº¦å¥–åŠ±å‡½æ•°
- **ç­”æ¡ˆå‡†ç¡®æ€§** (`accuracy`)ï¼šåŸºäº`math-verify`çš„ç²¾ç¡®éªŒè¯
- **æ ¼å¼è§„èŒƒæ€§** (`format`)ï¼šæ£€æŸ¥`<think>`å’Œ`<answer>`æ ‡ç­¾ç»“æ„
- **æ¨ç†æ­¥éª¤** (`reasoning_steps`)ï¼šè¯„ä¼°é€»è¾‘æ¨ç†è¿‡ç¨‹è´¨é‡
- **æ ‡ç­¾å®Œæ•´æ€§** (`tag_count`)ï¼šç¡®ä¿æ­£ç¡®çš„æ ‡ç­¾ä½¿ç”¨
- **æ–‡æœ¬é•¿åº¦** (`length`)ï¼šæ§åˆ¶ç”Ÿæˆé•¿åº¦çš„åˆç†æ€§
- **æ•°å­¦ä¸¥è°¨æ€§** (`mathematical_rigor`)ï¼šä¸“é—¨é’ˆå¯¹æ•°å­¦æ¨ç†çš„è¯„ä¼°

### âš™ï¸ çµæ´»é…ç½®ç³»ç»Ÿ
- **YAMLé©±åŠ¨**ï¼šæ‰€æœ‰è®­ç»ƒå‚æ•°é€šè¿‡é…ç½®æ–‡ä»¶ç®¡ç†
- **æƒé‡å¯è°ƒ**ï¼šæ¯ä¸ªå¥–åŠ±å‡½æ•°æ”¯æŒç‹¬ç«‹æƒé‡è®¾ç½®
- **åŠ¨æ€åŠ è½½**ï¼šæ™ºèƒ½åˆ†ç¦»è®­ç»ƒå‚æ•°å’Œæ¨¡å‹å‚æ•°
- **ç¯å¢ƒé€‚é…**ï¼šæ”¯æŒå¤šç§åŠ é€Ÿå™¨å’Œåˆ†å¸ƒå¼è®­ç»ƒ

### ğŸ”§ å¼€å‘è€…å‹å¥½
- **å®Œæ•´å·¥å…·é“¾**ï¼šæ—¥å¿—ã€æ ¼å¼åŒ–ã€æ•°æ®å¤„ç†ç­‰å®ç”¨å·¥å…·
- **æµ‹è¯•æ¡†æ¶**ï¼šå…¨é¢çš„åŠŸèƒ½æµ‹è¯•å’ŒéªŒè¯
- **è°ƒè¯•æ”¯æŒ**ï¼šè¯¦ç»†çš„æ—¥å¿—è®°å½•å’Œè°ƒè¯•æ¨¡å¼

## ğŸ“¦ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒå‡†å¤‡
```bash
# è¦æ±‚Python 3.10.9+
conda create -n r1 python=3.10
conda activate r1

# å…‹éš†é¡¹ç›®
git clone <repository-url>
cd r1

# å®‰è£…ä¾èµ–ï¼ˆåŸºäºPyTorchç”Ÿæ€ç³»ç»Ÿï¼‰
pip install -e .
```

### åŸºç¡€è®­ç»ƒ
```bash
# ä½¿ç”¨é¢„è®¾é…ç½®å¿«é€Ÿå¼€å§‹
python train_enhanced_demo.py

# è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
python train_enhanced_demo.py --config recipes/Qwen2.5-1.5B-Instruct/grpo/config_enhanced_demo.yaml

# è°ƒè¯•æ¨¡å¼ï¼ˆä¸æ‰§è¡Œå®é™…è®­ç»ƒï¼‰
python train_enhanced_demo.py --debug --dry-run
```

### é«˜çº§è®­ç»ƒ
```bash
# ä½¿ç”¨åŸç”ŸTRLæ¥å£
python src/r1/grpo.py --config <config_file>

# SFTé¢„è®­ç»ƒ
python src/r1/sft.py --config <sft_config_file>
```

## ğŸ“‹ é…ç½®è¯¦è§£

### åŸºç¡€é…ç½®ç¤ºä¾‹
```yaml
# æ¨¡å‹è®¾ç½®
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
torch_dtype: bfloat16

# æ•°æ®é›†é…ç½®
dataset_name: open-r1/Mixture-of-Thoughts
dataset_prompt_column: prompt

# è®­ç»ƒå‚æ•°
learning_rate: 3.0e-6
num_train_epochs: 2
per_device_train_batch_size: 2
gradient_accumulation_steps: 4
max_completion_length: 2048

# åŸºç¡€å¥–åŠ±å‡½æ•°
reward_funcs:
  - accuracy
  - format
  - tag_count
reward_weights: [1.0, 1.0, 1.0]
```

### å¢å¼ºåŠŸèƒ½é…ç½®
```yaml
# å®æ—¶è¯„ä¼° ğŸ”
eval_steps: 50                    # æ¯50æ­¥è¯„ä¼°ä¸€æ¬¡
eval_on_start: true              # è®­ç»ƒå‰è¯„ä¼°
max_eval_samples: 100            # è¯„ä¼°æ ·æœ¬æ•°é™åˆ¶
eval_dataset_name: gsm8k         # ç‹¬ç«‹è¯„ä¼°æ•°æ®é›†

# æ‹’ç»é‡‡æ · ğŸ²
use_reject_sampling: true        # å¯ç”¨æ‹’ç»é‡‡æ ·
max_resample_attempts: 3         # æœ€å¤§é‡è¯•æ¬¡æ•°
reject_sampling_threshold: 0.6   # è´¨é‡é˜ˆå€¼

# å¤šç»´åº¦å¥–åŠ± ğŸ†
reward_funcs:
  - accuracy           # ç­”æ¡ˆå‡†ç¡®æ€§
  - format            # æ ¼å¼è§„èŒƒ
  - reasoning_steps   # æ¨ç†è´¨é‡
  - mathematical_rigor # æ•°å­¦ä¸¥è°¨æ€§
reward_weights: [4.0, 1.0, 2.0, 1.5]  # æƒé‡é…ç½®

# æ€§èƒ½ä¼˜åŒ– âš¡
torch_empty_cache_steps: 5       # GPUç¼“å­˜æ¸…ç†é¢‘ç‡
gradient_checkpointing: true     # æ¢¯åº¦æ£€æŸ¥ç‚¹
use_liger_kernel: true          # Ligerä¼˜åŒ–å†…æ ¸

# ç›‘æ§æ—¥å¿— ğŸ“Š
wandb_project: R1-Training
log_completion_details: true
save_completions: false
debug_mode: true
```

## ğŸ—ï¸ é¡¹ç›®æ¶æ„

```
r1/
â”œâ”€â”€ ğŸ“ src/r1/                    # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ ğŸ“„ configs.py            # é…ç½®ç±»å®šä¹‰ï¼ˆ16KB, 448è¡Œï¼‰
â”‚   â”œâ”€â”€ ğŸ“„ enhanced_grpo_trainer.py  # å¢å¼ºç‰ˆGRPOè®­ç»ƒå™¨ï¼ˆ10KB, 284è¡Œï¼‰
â”‚   â”œâ”€â”€ ğŸ“„ trainer.py            # è®­ç»ƒå™¨æ¥å£
â”‚   â”œâ”€â”€ ğŸ“„ rewards.py            # å¥–åŠ±å‡½æ•°åº“ï¼ˆ13KB, 354è¡Œï¼‰
â”‚   â”œâ”€â”€ ğŸ“„ callbacks.py          # è®­ç»ƒå›è°ƒ
â”‚   â”œâ”€â”€ ğŸ“„ grpo.py               # GRPOè®­ç»ƒè„šæœ¬ï¼ˆ6.5KB, 183è¡Œï¼‰
â”‚   â”œâ”€â”€ ğŸ“„ sft.py                # SFTè®­ç»ƒè„šæœ¬ï¼ˆ6KB, 172è¡Œï¼‰
â”‚   â””â”€â”€ ğŸ“ utils/                # å·¥å…·åº“
â”‚       â”œâ”€â”€ ğŸ“„ logging.py        # æ—¥å¿—ç³»ç»Ÿï¼ˆ4.6KB, 152è¡Œï¼‰
â”‚       â”œâ”€â”€ ğŸ“„ formatting.py     # æ–‡æœ¬æ ¼å¼åŒ–ï¼ˆ3.6KB, 152è¡Œï¼‰
â”‚       â”œâ”€â”€ ğŸ“„ data.py           # æ•°æ®å¤„ç†ï¼ˆ2.6KB, 66è¡Œï¼‰
â”‚       â”œâ”€â”€ ğŸ“„ model_utils.py    # æ¨¡å‹å·¥å…·ï¼ˆ1.6KB, 43è¡Œï¼‰
â”‚       â”œâ”€â”€ ğŸ“„ import_utils.py   # ä¾èµ–æ£€æŸ¥ï¼ˆ1.4KB, 55è¡Œï¼‰
â”‚       â”œâ”€â”€ ğŸ“„ callbacks.py      # å›è°ƒå·¥å…·ï¼ˆ938B, 30è¡Œï¼‰
â”‚       â””â”€â”€ ğŸ“„ wandb_logging.py  # W&Bé›†æˆï¼ˆ480B, 14è¡Œï¼‰
â”œâ”€â”€ ğŸ“ recipes/                  # é…ç½®æ¨¡æ¿
â”‚   â””â”€â”€ ğŸ“ Qwen2.5-1.5B-Instruct/grpo/
â”‚       â”œâ”€â”€ ğŸ“„ config_demo.yaml          # åŸºç¡€é…ç½®
â”‚       â””â”€â”€ ğŸ“„ config_enhanced_demo.yaml # å¢å¼ºé…ç½®
â”œâ”€â”€ ğŸ“„ train_enhanced_demo.py     # è®­ç»ƒæ¼”ç¤ºè„šæœ¬ï¼ˆ3KB, 92è¡Œï¼‰
â”œâ”€â”€ ğŸ“„ test_enhanced_features.py  # åŠŸèƒ½æµ‹è¯•è„šæœ¬ï¼ˆ4.8KB, 164è¡Œï¼‰
â””â”€â”€ ğŸ“„ setup.py                  # åŒ…é…ç½®ï¼ˆ3.7KB, 125è¡Œï¼‰
```

## ğŸ“Š å¥–åŠ±å‡½æ•°è¯¦è§£

### ğŸ¯ accuracy - ç­”æ¡ˆå‡†ç¡®æ€§
- ä½¿ç”¨`math-verify`å’Œ`latex2sympy2_extended`è¿›è¡Œç²¾ç¡®éªŒè¯
- æ”¯æŒå¤šç§æ•°å­¦è¡¨è¾¾å¼æ ¼å¼
- åŒ…å«ç½®ä¿¡åº¦å¥–åŠ±æœºåˆ¶

### ğŸ“ format - æ ¼å¼è§„èŒƒæ€§
- éªŒè¯`<think>`å’Œ`<answer>`æ ‡ç­¾å®Œæ•´æ€§
- æ£€æŸ¥æ€è€ƒè¿‡ç¨‹å’Œç­”æ¡ˆçš„ç»“æ„åˆç†æ€§
- å¥–åŠ±æ¸…æ™°çš„é€»è¾‘åˆ†æ®µ

### ğŸ§  reasoning_steps - æ¨ç†æ­¥éª¤
- æ£€æµ‹æ­¥éª¤æŒ‡ç¤ºè¯ï¼ˆStep 1, First, Thenç­‰ï¼‰
- è¯„ä¼°é€»è¾‘è¿‡æ¸¡è¯çš„ä½¿ç”¨
- æ”¯æŒä¸­è‹±æ–‡æ¨ç†æ¨¡å¼

### ğŸ·ï¸ tag_count - æ ‡ç­¾å®Œæ•´æ€§
- ç²¾ç¡®è®¡ç®—æ ‡ç­¾æ•°é‡
- æƒ©ç½šå¤šä½™æˆ–ç¼ºå¤±çš„æ ‡ç­¾
- ç¡®ä¿æ ¼å¼ä¸€è‡´æ€§

### ğŸ“ length - æ–‡æœ¬é•¿åº¦
- åŸºäºé«˜æ–¯åˆ†å¸ƒçš„é•¿åº¦å¥–åŠ±
- å¯é…ç½®ç›®æ ‡é•¿åº¦èŒƒå›´
- é¿å…è¿‡çŸ­æˆ–è¿‡é•¿çš„ç”Ÿæˆ

### ğŸ”¬ mathematical_rigor - æ•°å­¦ä¸¥è°¨æ€§
- æ£€æµ‹æ•°å­¦æœ¯è¯­å’Œç¬¦å·
- è¯„ä¼°é€»è¾‘ç»“æ„å®Œæ•´æ€§
- ä¸“ä¸ºæ•°å­¦æ¨ç†ä»»åŠ¡ä¼˜åŒ–

## ğŸš€ é«˜çº§åŠŸèƒ½

### å®æ—¶è¯„ä¼°ç³»ç»Ÿ
```python
# è‡ªåŠ¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°æ¨¡å‹
trainer = EnhancedGRPOTrainer(
    eval_dataset=eval_data,
    eval_steps=50,
    eval_on_start=True,
    max_eval_samples=100
)
```

### æ‹’ç»é‡‡æ ·æœºåˆ¶
```python
# æ™ºèƒ½é‡é‡‡æ ·æå‡ç”Ÿæˆè´¨é‡
completions = trainer._generate_completions_with_reject_sampling(
    prompts=prompts,
    max_resample_attempts=3,
    reject_sampling_threshold=0.6
)
```

### æ¨¡å—åŒ–å¥–åŠ±å‡½æ•°
```python
# åˆ›å»ºè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°ç»„åˆ
from src.r1.rewards import get_reward_calculator

calculator = get_reward_calculator(
    reward_funcs=['accuracy', 'format', 'mathematical_rigor'],
    reward_weights=[4.0, 1.0, 2.0]
)
```

## ğŸ“ˆ ç›‘æ§ä¸è°ƒè¯•

### Weights & Biasesé›†æˆ
```yaml
# å®Œæ•´çš„å®éªŒè·Ÿè¸ª
wandb_project: R1-Experiments
wandb_entity: your-team
run_name: qwen-grpo-v1
report_to: [wandb]
```

### è¯¦ç»†æ—¥å¿—ç³»ç»Ÿ
```python
# å¤šçº§åˆ«æ—¥å¿—è®°å½•
from src.r1.utils.logging import setup_logger, TrainingLogger

logger = setup_logger("r1_training", level=logging.DEBUG)
training_logger = TrainingLogger(log_file="training.log")
```

### åŠŸèƒ½æµ‹è¯•
```bash
# è¿è¡Œå…¨é¢çš„åŠŸèƒ½æµ‹è¯•
python test_enhanced_features.py

# è¾“å‡ºç¤ºä¾‹ï¼š
# ğŸ¯ æ€»ä½“ç»“æœ: ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!
```

## ğŸ› ï¸ æŠ€æœ¯æ ˆ

### æ ¸å¿ƒä¾èµ–
- **PyTorch 2.6.0** - æ·±åº¦å­¦ä¹ æ¡†æ¶
- **Transformers 4.52.3** - æ¨¡å‹åº“
- **TRL 0.18.0** - å¼ºåŒ–å­¦ä¹ è®­ç»ƒ
- **Accelerate 1.4.0** - åˆ†å¸ƒå¼è®­ç»ƒ
- **DeepSpeed 0.16.8** - å†…å­˜ä¼˜åŒ–

### ä¸“ä¸šå·¥å…·
- **math-verify 0.5.2** - æ•°å­¦éªŒè¯
- **latex2sympy2_extended** - LaTeXè§£æ
- **liger-kernel** - GPUä¼˜åŒ–
- **bitsandbytes** - é‡åŒ–è®­ç»ƒ

## ğŸ“ å¼€å‘æŒ‡å—

### æ·»åŠ æ–°å¥–åŠ±å‡½æ•°
```python
def custom_reward(completions, **kwargs) -> list[float]:
    """è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°"""
    # å®ç°è¯„ä¼°é€»è¾‘
    return scores

# æ³¨å†Œåˆ°ç³»ç»Ÿ
REWARD_FUNCS_REGISTRY["custom"] = custom_reward
```

### è‡ªå®šä¹‰é…ç½®
```python
from src.r1.configs import GRPOScriptArguments

@dataclass
class CustomConfig(GRPOScriptArguments):
    custom_param: float = field(default=1.0)
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. **Forké¡¹ç›®** å¹¶åˆ›å»ºç‰¹æ€§åˆ†æ”¯
2. **è¿è¡Œæµ‹è¯•** ç¡®ä¿åŠŸèƒ½æ­£å¸¸ï¼š`python test_enhanced_features.py`
3. **éµå¾ªä»£ç é£æ ¼** ä½¿ç”¨blackå’Œisortæ ¼å¼åŒ–
4. **æäº¤PR** å¹¶æè¿°å˜æ›´å†…å®¹

## ğŸ“„ è®¸å¯è¯

Apache License 2.0 - è¯¦è§LICENSEæ–‡ä»¶

---

> ğŸ’¡ **ä¸“æ³¨äºæ¨ç†ä»»åŠ¡çš„è½»é‡çº§GRPOæ¡†æ¶ï¼Œè®©è®­ç»ƒé«˜è´¨é‡æ¨ç†æ¨¡å‹å˜å¾—ç®€å•é«˜æ•ˆï¼**

</div>

<script>
function switchLang(lang) {
    // Hide all language content
    document.querySelectorAll('.lang-content').forEach(el => {
        el.classList.remove('active');
    });
    
    // Show selected language content
    document.getElementById('lang-' + lang).classList.add('active');
    
    // Update button states
    document.querySelectorAll('.lang-btn').forEach(btn => {
        btn.classList.remove('active');
    });
    event.target.classList.add('active');
    
    // Update page title and lang attribute
    if (lang === 'zh') {
        document.title = 'R1 - è½»é‡çº§GRPOæ¨ç†æ¨¡å‹è®­ç»ƒæ¡†æ¶';
        document.documentElement.lang = 'zh-CN';
    } else {
        document.title = 'R1 - Lightweight GRPO Inference Model Training Framework';
        document.documentElement.lang = 'en';
    }
}
</script>

</body>
</html>
